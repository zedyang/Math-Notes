
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    %\usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath,bm,bbm} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Machine Learning HW III}
    \author{Ze Yang (zey@andrew.cmu.edu)}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{sklearn} \PY{k}{as} \PY{n+nn}{sk}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{n}{plt}\PY{o}{.}\PY{n}{style}\PY{o}{.}\PY{n}{use}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ggplot}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        
        \PY{k+kn}{import} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{stats}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{mean\PYZus{}squared\PYZus{}error}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{LinearRegression}
        \PY{k+kn}{from} \PY{n+nn}{pygam} \PY{k}{import} \PY{n}{LinearGAM}
        \PY{k+kn}{from} \PY{n+nn}{pygam}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k}{import} \PY{n}{generate\PYZus{}X\PYZus{}grid}
        \PY{k+kn}{from} \PY{n+nn}{copy} \PY{k}{import} \PY{n}{copy}
        \PY{k+kn}{from} \PY{n+nn}{progressbar} \PY{k}{import} \PY{n}{ProgressBar}
\end{Verbatim}

    \section{Forward Rate}\label{forward-rate}

    \subsection{Smooth Spline Fit of the Forward
Rate}\label{smooth-spline-fit-of-the-forward-rate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}98}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{capture} \PYZhy{}\PYZhy{}no\PYZhy{}stdout \PYZhy{}\PYZhy{}no\PYZhy{}display
         gam = LinearGAM().gridsearch(x,y)
         
         fig, ax = plt.subplots(1, 1, figsize=(10,10))
         x\PYZus{}grid = np.linspace(0,30,100).reshape(100,1)
         ax.plot(x,y,\PYZsq{}o\PYZhy{}\PYZhy{}\PYZsq{}, alpha=0.4, label=\PYZsq{}sample\PYZsq{})
         ax.plot(x\PYZus{}grid, gam.predict(x\PYZus{}grid), linewidth=2, label=\PYZsq{}fitted\PYZsq{})
         ax.update(\PYZob{}\PYZsq{}xlabel\PYZsq{}:\PYZsq{}Time\PYZsq{}, \PYZsq{}ylabel\PYZsq{}:\PYZsq{}Rate\PYZsq{}, 
                    \PYZsq{}title\PYZsq{}:\PYZsq{}Forward Rates Over Time and the Smoothing Spline Fit\PYZsq{}\PYZcb{})
         \PYZus{} = ax.legend()
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
100\% (11 of 11) |\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#| Elapsed Time: 0:00:00 Time: 0:00:00

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_3_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Yes, we obtain a fairly believable smooth fit.

    \subsection{Splines are Nice for
Integrals}\label{splines-are-nice-for-integrals}

    The advantage of smoothing splines is that they are essentially
piecewise cubic polynomial, which we can write down an explicit formula
for (knowing the basis functions and the parameters with which they are
linearly combined).

Then, integrating cublic polynomials becomes a piece of cake. We can
just take advantage the linearity of integral: evaluate the integration
for each piece of the spline, then add them together.

    \section{Kernel Regression and Varying
Smoothness}\label{kernel-regression-and-varying-smoothness}

    \subsection{\texorpdfstring{\(y\) Versus
\(x\)}{y Versus x}}\label{y-versus-x}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{k}{def} \PY{n+nf}{mu}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    ground truth: }
         \PY{l+s+sd}{    y = \PYZob{} sin(x/2) on [0,4*pi] }
         \PY{l+s+sd}{          sin(6*x) on [4*pi,8*pi] \PYZcb{}}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{c+c1}{\PYZsh{} }
             \PY{n}{y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
             \PY{n}{lefts}\PY{p}{,} \PY{n}{rights} \PY{o}{=} \PY{p}{(}\PY{n}{x}\PY{o}{\PYZlt{}}\PY{o}{=}\PY{l+m+mi}{4}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{x}\PY{o}{\PYZgt{}}\PY{l+m+mi}{4}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{p}{)}
             \PY{n}{y}\PY{p}{[}\PY{n}{lefts}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{n}{lefts}\PY{p}{]}\PY{o}{/}\PY{l+m+mf}{2.0}\PY{p}{)}
             \PY{n}{y}\PY{p}{[}\PY{n}{rights}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{l+m+mi}{6}\PY{o}{*}\PY{n}{x}\PY{p}{[}\PY{n}{rights}\PY{p}{]}\PY{p}{)}
             \PY{k}{return} \PY{n}{y}
         
         \PY{k}{def} \PY{n+nf}{generate\PYZus{}sample}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} sample X uniformly on [0,8*pi].}
             \PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{uniform}\PY{p}{(}\PY{n}{low}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{high}\PY{o}{=}\PY{l+m+mi}{8}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{n}\PY{p}{)}
             \PY{n}{x}\PY{o}{.}\PY{n}{sort}\PY{p}{(}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} sample Y as gaussians around mu(x)}
             \PY{n}{y} \PY{o}{=} \PY{n}{mu}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{scale}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{n}\PY{p}{)}
             \PY{k}{return} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{x}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{y}\PY{p}{\PYZcb{}}\PY{p}{)}
         
         \PY{n}{scipy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{7}\PY{p}{)}
         \PY{n}{sine\PYZus{}df} \PY{o}{=} \PY{n}{generate\PYZus{}sample}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}101}]:} \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{18}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
          \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{sine\PYZus{}df}\PY{o}{.}\PY{n}{x}\PY{p}{,}\PY{n}{sine\PYZus{}df}\PY{o}{.}\PY{n}{y}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.7}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sample}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{sine\PYZus{}df}\PY{o}{.}\PY{n}{x}\PY{p}{,}\PY{n}{mu}\PY{p}{(}\PY{n}{sine\PYZus{}df}\PY{o}{.}\PY{n}{x}\PY{p}{)}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{True \PYZdl{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{mu(x)\PYZdl{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{ax}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{xlabel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ylabel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{title}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Random Sample and the True \PYZdl{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{mu(}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{cdot)\PYZdl{} Function}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}\PY{p}{)}
          \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_10_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The two parts of the data are very different. For \(x>4 \pi\), \(\mu\)
is very wiggly. For \(x\leq 4\pi\), it's a lot smoother.

    \subsection{Different Parts of Data for
CV}\label{different-parts-of-data-for-cv}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}90}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{capture} \PYZhy{}\PYZhy{}no\PYZhy{}stdout \PYZhy{}\PYZhy{}no\PYZhy{}display
         left\PYZus{}df = sine\PYZus{}df[sine\PYZus{}df.x \PYZlt{}= 4*np.pi]
         right\PYZus{}df = sine\PYZus{}df[sine\PYZus{}df.x \PYZgt{} 4*np.pi]
         cv\PYZus{}lams = np.zeros(3)
         
         for i, df in enumerate([left\PYZus{}df, right\PYZus{}df, sine\PYZus{}df]):
             x, y = df.x.values, df.y.values
             gam = LinearGAM(n\PYZus{}splines=100).gridsearch(
                 x, y, lam=np.logspace(\PYZhy{}5, 4, 100))
             cv\PYZus{}lams[i] = gam.lam
         print(\PYZsq{}Selected lambdas: \PYZsq{}, cv\PYZus{}lams)
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
100\% (100 of 100) |\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#| Elapsed Time: 0:00:02 Time: 0:00:02
100\% (100 of 100) |\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#| Elapsed Time: 0:00:02 Time: 0:00:02
100\% (100 of 100) |\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#| Elapsed Time: 0:00:03 Time: 0:00:03

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Selected lambdas:  [  1.87381742e+03   3.51119173e-01   2.84803587e-02]

    \end{Verbatim}

    The optimal \(\lambda\)'s are printed above. Let's call them
\(\lambda_{x\leq 4\pi}\), \(\lambda_{x> 4\pi}\) and \(\lambda_{all}\).
We have

\[
\lambda_{x\leq 4\pi} > \lambda_{x\leq 4\pi} > \lambda_{all}
\]

This implies that fitting on the first part of data penalizes the most
on the wiggliness of \(\mu\), while fitting on the entire data set
penalizes not that much, which results in a more flexible function.

    \subsection{Varying Performance}\label{varying-performance}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}115}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{capture} \PYZhy{}\PYZhy{}no\PYZhy{}stdout \PYZhy{}\PYZhy{}no\PYZhy{}display
          adj\PYZus{}lams = copy(cv\PYZus{}lams); adj\PYZus{}lams[[0,1]] /= 2
          for i, desc in enumerate([
              \PYZsq{}\PYZdl{}x\PYZbs{}leq 4\PYZbs{}pi\PYZdl{} Part\PYZsq{}, \PYZsq{}\PYZdl{}x\PYZgt{}4\PYZbs{}pi\PYZdl{} Part\PYZsq{}, \PYZsq{}All\PYZsq{}]):
              x, y = sine\PYZus{}df.x.values, sine\PYZus{}df.y.values
              gam = LinearGAM(
                  n\PYZus{}splines=100, 
                  lam=adj\PYZus{}lams[i]).fit(x, y)
              
              fig, ax = plt.subplots(1, 1, figsize=(18,6))
              x\PYZus{}grid = np.linspace(0,8*np.pi,500).reshape(500,1)
              ax.plot(x,y,\PYZsq{}o\PYZsq{}, alpha=0.7, label=\PYZsq{}Sample\PYZsq{})
              ax.plot(x\PYZus{}grid, gam.predict(x\PYZus{}grid), 
                      linewidth=3, label=\PYZsq{}Fitted\PYZsq{})
              ax.plot(x\PYZus{}grid, mu(x\PYZus{}grid.reshape(500,)), 
                      \PYZsq{}\PYZhy{}\PYZhy{}\PYZsq{}, linewidth=3, label=\PYZsq{}True\PYZsq{})
              ax.update(\PYZob{}
                  \PYZsq{}xlabel\PYZsq{}:\PYZsq{}x\PYZsq{}, 
                  \PYZsq{}ylabel\PYZsq{}:\PYZsq{}y\PYZsq{}, 
                  \PYZsq{}title\PYZsq{}:\PYZsq{}Sample and the Fitted Smoothing Spline \PYZsq{}+
                  r\PYZsq{}\PYZdl{}\PYZbs{}lambda=\PYZob{}\PYZcb{}\PYZdl{}, Estimated from \PYZob{}\PYZcb{} of the Data\PYZsq{}.format(
                      round(adj\PYZus{}lams[i],3), desc)
              \PYZcb{})
              ax.legend()
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_16_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_16_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_16_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Comparison}\label{comparison}

\begin{itemize}
\item
  The regression fitted on the \textbf{left} half of the data
  \textbf{underfits} the right half. As we seen in previous section,
  \(\lambda_{x\leq 4\pi}\) turns out to be very large, and it demands a
  very flat spline fit, which is insufficient for the wiggly part of the
  data.
\item
  The regression fitted on the \textbf{right} half of the data slightly
  \textbf{overfits} the left half. If we zoom in to the second plot, we
  will see many wiggles in the left part of the curve, which, of course,
  is a result of the small \(\lambda_{x< 4\pi}\).
\item
  The regression fitted on the whole data set seems to do well on the
  right half, but it still \textbf{overfits} the left half. In fact it
  is even more than the second case, since \(\lambda_{all}\) is even
  smaller. This is an interesting result. I think it is because the
  parameter \(\lambda\) is applied \textbf{Globally} to the optimization
  problem. Namely, if the sample is wiggly in a significant proportion
  of it, and the overall variance is not that large, then globally
  applying an large lambda turns out to not be a good idea. As a result,
  a small lambda will be chosen, and the overall fit will be more
  adapted to the most wiggly part of the function, making some
  compromise to the variance on its relatively flat part. But this might
  not be always true, there is always a trade-off.
\end{itemize}

    \section{Additive Models for
Prediction}\label{additive-models-for-prediction}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{bike\PYZus{}train.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{bike\PYZus{}test.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{X\PYZus{}names}\PY{p}{,} \PY{n}{y\PYZus{}name} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{train}\PY{o}{.}\PY{n}{columns}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PY{n}{train}\PY{o}{.}\PY{n}{columns}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{values}
        \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{values}
        \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{test}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{values}
        \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{test}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{values}
\end{Verbatim}

    \subsection{Linear Regression MSE}\label{linear-regression-mse}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{lm} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
        \PY{n}{y\PYZus{}pred\PYZus{}lm} \PY{o}{=} \PY{l+m+mf}{1.6}\PY{o}{*}\PY{n}{lm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
        \PY{n}{mse\PYZus{}lm} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred\PYZus{}lm}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MSE of linear regression is:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{mse\PYZus{}lm}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
MSE of linear regression is: 1295159.39451

    \end{Verbatim}

    \subsection{Linear GAM MSE}\label{linear-gam-mse}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{capture} \PYZhy{}\PYZhy{}no\PYZhy{}stdout \PYZhy{}\PYZhy{}no\PYZhy{}display
        gam = LinearGAM().gridsearch(
            X\PYZus{}train,y\PYZus{}train,lam=np.logspace(\PYZhy{}5, 4, 100))
        y\PYZus{}pred\PYZus{}gam = 1.6*gam.predict(X\PYZus{}test)
        mse\PYZus{}gam = mean\PYZus{}squared\PYZus{}error(y\PYZus{}test, y\PYZus{}pred\PYZus{}gam)
        print(\PYZsq{}MSE of linear GAM is:\PYZsq{}, mse\PYZus{}gam)
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
100\% (100 of 100) |\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#| Elapsed Time: 0:00:05 Time: 0:00:05

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
MSE of linear GAM is: 967944.688091

    \end{Verbatim}

    \subsection{Partial Dependence of Linear
GAM}\label{partial-dependence-of-linear-gam}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{X\PYZus{}grid} \PY{o}{=} \PY{n}{generate\PYZus{}X\PYZus{}grid}\PY{p}{(}\PY{n}{gam}\PY{p}{)}
        \PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{off}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;} \PY{n}{axes}\PY{o}{=}\PY{n}{axes}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{)}
        
        \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{ax} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}names}\PY{p}{)}\PY{p}{]}\PY{p}{)}\PY{p}{:}
            \PY{n}{pdep}\PY{p}{,} \PY{n}{confi} \PY{o}{=} \PY{n}{gam}\PY{o}{.}\PY{n}{partial\PYZus{}dependence}\PY{p}{(}
                \PY{n}{X\PYZus{}grid}\PY{p}{,} \PY{n}{feature}\PY{o}{=}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{width}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{95}\PY{p}{)}
            \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{X\PYZus{}grid}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{pdep}\PY{p}{)}
            \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{n}{X\PYZus{}names}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
        \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Partial Dependence Plots}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The partial dependence to \texttt{dayofyear}, \texttt{temp},
\texttt{humidity} has very obvious nonlinear shapes. These are the
relationships that we are mostly likely to miss when using linear
models.\\
~\\
\textbf{Interpretations}: 
\begin{itemize}
    \item \texttt{dayofyear}: The bike rental is
active in the middle of the year, less active in winter and during holiday.
\item  \texttt{dayofweek}: Bike rental has an irregular distribution
in a week, tend to be more of it on Saturday, much less on Monday.
\item \texttt{workday}: In general, there's more bike rentals on weekdays than
on weekends. 
\item \texttt{weathertype}: More bike rental on sunny days,
less during the bad weathers. 
\item \texttt{temp}: Less bike rental when
it's either too cold or too hot. 
\item \texttt{humidity}: Less bike rental
when it's either too damp or too dry, which can be direct consequences
of rainy/hot weathers.
\item \texttt{windspeed}: Bike rental decrease with
windspeed. A whole lot lessLess bike rental when it's blustery.
\end{itemize}


    \subsection{True Loss}\label{true-loss}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{k}{def} \PY{n+nf}{real\PYZus{}loss}\PY{p}{(}\PY{n}{true}\PY{p}{,} \PY{n}{guess}\PY{p}{)}\PY{p}{:}
            \PY{k}{return}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}
                \PY{n}{true}\PY{o}{\PYZgt{}}\PY{n}{guess}\PY{p}{,} \PY{l+m+mi}{5}\PY{o}{*}\PY{p}{(}\PY{n}{true}\PY{o}{\PYZhy{}}\PY{n}{guess}\PY{p}{)}\PY{p}{,} \PY{n}{guess}\PY{o}{\PYZhy{}}\PY{n}{true}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{L\PYZus{}lm} \PY{o}{=} \PY{n}{real\PYZus{}loss}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred\PYZus{}lm}\PY{p}{)}
        \PY{n}{L\PYZus{}gam} \PY{o}{=} \PY{n}{real\PYZus{}loss}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred\PYZus{}gam}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Real loss of linear regression is:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{L\PYZus{}lm}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Real loss of linear GAM is:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{L\PYZus{}gam}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Real loss of linear regression is: 2412.83028509
Real loss of linear GAM is: 2075.47448558

    \end{Verbatim}

    \subsection{Quantile Adjustment}\label{quantile-adjustment}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{residual\PYZus{}lm} \PY{o}{=} \PY{n}{y\PYZus{}test} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}pred\PYZus{}lm}
        \PY{n}{residual\PYZus{}gam} \PY{o}{=} \PY{n}{y\PYZus{}test} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}pred\PYZus{}gam}
        \PY{n}{adj\PYZus{}lm} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{percentile}\PY{p}{(}\PY{n}{residual\PYZus{}lm}\PY{p}{,} \PY{l+m+mi}{100}\PY{o}{*}\PY{l+m+mi}{5}\PY{o}{/}\PY{l+m+mi}{6}\PY{p}{)}
        \PY{n}{adj\PYZus{}gam} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{percentile}\PY{p}{(}\PY{n}{residual\PYZus{}gam}\PY{p}{,} \PY{l+m+mi}{100}\PY{o}{*}\PY{l+m+mi}{5}\PY{o}{/}\PY{l+m+mi}{6}\PY{p}{)}
        \PY{n}{y\PYZus{}pred\PYZus{}lm\PYZus{}adj} \PY{o}{=} \PY{n}{y\PYZus{}pred\PYZus{}lm} \PY{o}{+} \PY{n}{adj\PYZus{}lm}
        \PY{n}{y\PYZus{}pred\PYZus{}gam\PYZus{}adj} \PY{o}{=} \PY{n}{y\PYZus{}pred\PYZus{}gam} \PY{o}{+} \PY{n}{adj\PYZus{}gam}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{L\PYZus{}lm\PYZus{}adj} \PY{o}{=} \PY{n}{real\PYZus{}loss}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred\PYZus{}lm\PYZus{}adj}\PY{p}{)}
         \PY{n}{L\PYZus{}gam\PYZus{}adj} \PY{o}{=} \PY{n}{real\PYZus{}loss}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred\PYZus{}gam\PYZus{}adj}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Real loss of linear regression (quantile adjusted) is:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{L\PYZus{}lm\PYZus{}adj}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Real loss of linear GAM (quantile adjusted) is:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{L\PYZus{}gam\PYZus{}adj}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Real loss of linear regression (quantile adjusted) is: 1429.4534003
Real loss of linear GAM (quantile adjusted) is: 1354.27558337

    \end{Verbatim}

    The performance of both estimators increases compared to the
non-adjusted version of them. Yet the GAM estimator still outperforms
the linear regression estimator. Therefore, my final choice is the
quantile-adjusted GAM estimator.

    \section{How Splines are Fitted}\label{how-splines-are-fitted}

    \textbf{(a)}, \textbf{(b)} Let $\bm{B}$ be $n\times (K+1)$ matrix, where $\{\bm{B}\}_{ik} = b_k(x_i)$, and define $b_0(\cdot)\equiv 1$. Then we can define $\hat{\bm{y}}$ as we did for linear regression
\begin{equation}
    \hat{\bm{y}} = \begin{pmatrix}
        f(x_1) & \hdots &f(x_n)
    \end{pmatrix}^{\top} = \begin{pmatrix}
        \sum_{k=0}^{K}\beta_k b_k(x_1) & \hdots & \sum_{k=0}^{K}\beta_k b_k(x_n)
    \end{pmatrix}^{\top} = \bm{B\beta}
\end{equation}
Therefore
\begin{equation}
    \sum_{i=1}^n (y_i - f(x_i))^2 = (\bm{y}-\hat{\bm{y}})^{\top} (\bm{y}-\hat{\bm{y}}) = (\bm{y} - \bm{B\beta})^{\top} (\bm{y} - \bm{B\beta})
\end{equation}
Meanwhile, we have $f''(x_i) = \sum_{k=0}^{K}\beta_k b_k''(x_i) = \bm{B}''\bm{\beta}$, where $\bm{B}''$ is a $n\times (K+1)$ matrix, $\{\bm{B}''\}_{ik} = b_k''(x_i)$. Hence
\begin{equation}
    \begin{split}
        \int (f''(x))^2 dx &= \int \bm{\beta}^{\top}\bm{B}''^{\top} \bm{B}''\bm{\beta} dx \\
        &= \bm{\beta}^{\top} \left(\int \bm{B}''^{\top} \bm{B}'' dx\right) \bm{\beta} \\
        &= \bm{\beta}^{\top} \bm{\Omega}_{b} \bm{\beta}
    \end{split}
\end{equation}
We can move the integral sign around due to its linearity. And we define $\bm{\Omega}_b$ as a $(K+1)\times(K+1)$ matrix, $\{\bm{\Omega}\}_{ij} = \int b''_i(x) b''_j(x)dx$, which is the elementwise integral of $\bm{B}''^{\top} \bm{B}''$. So the optimization problem becomes one with respect to $\bm{\beta}$:
\begin{equation}
    \min\limits_{\bm{\beta}}~(\bm{y} - \bm{B\beta})^{\top} (\bm{y} - \bm{B\beta}) + \lambda \bm{\beta}^{\top} \bm{\Omega}_{b} \bm{\beta}
  \end{equation}  
\textbf{(c)} The same matrix augmentation argument in last homework will lead us to the closed form answer:
\begin{equation}
    \hat{\bm{\beta}} = ( \bm{B}^{\top} \bm{B} + \lambda \bm{\Omega}_b)^{-1} \bm{B}^{\top} \bm{y}
\end{equation}
Or we can also solve the first order condition directly, note that $\bm{\Omega}_b$ is symmetric, so 
$$
\frac{\partial \lambda \bm{\beta}^{\top} \bm{\Omega}_{b} \bm{\beta}}{\partial \bm{\beta}} = \lambda \bm{\Omega}_b \bm{\beta} + \lambda \bm{\Omega}_b^{\top} \bm{\beta} = 2\lambda \bm{\Omega}_b \bm{\beta}
$$
We then have:
\begin{equation}
    \begin{split}
        &\frac{\partial \mathcal{L}}{\partial \bm{\beta}} = -2 \bm{B}^{\top}(\bm{y}-\bm{B\beta}) + 2\lambda \bm{\Omega}_b \bm{\beta} = 0\\
        \Rightarrow ~~~& (\bm{B}^{\top} \bm{B} + \lambda \bm{\Omega}_b)\bm{\beta} =  \bm{B}^{\top} \bm{y} \\
        \Rightarrow~~~&\hat{\bm{\beta}} = ( \bm{B}^{\top} \bm{B} + \lambda \bm{\Omega}_b)^{-1} \bm{B}^{\top} \bm{y}
    \end{split}
\end{equation}
Note that this works if and only if $(\bm{B}^{\top} \bm{B} + \lambda \bm{\Omega}_b)$ is invertible. If we are free to assume that $\bm{\Omega}_b$ is positive definite, it is invertible. Moremore we can apply Cholesky decomposition and write $\bm{\Omega}_b = \bm{A}^{\top} \bm{A}$, for some triangular matrix $\bm{A}$, and the matrix augmentation argument will work.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
